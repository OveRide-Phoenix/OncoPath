{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbecdcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests pandas tensorflow scikit-learn opencv-python tqdm pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "182004f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pydicom import dcmread\n",
    "import requests\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34be3417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 1098 clinical records.\n",
      "                                     id                               case_id  \\\n",
      "0  3c612e12-6de8-44fa-a095-805c45474821  3c612e12-6de8-44fa-a095-805c45474821   \n",
      "1  3cb06c7a-f2a8-448b-91a8-dd201bbf2ddd  3cb06c7a-f2a8-448b-91a8-dd201bbf2ddd   \n",
      "2  3d676bba-154b-4d22-ab59-d4d4da051b94  3d676bba-154b-4d22-ab59-d4d4da051b94   \n",
      "3  dfaabd03-2d40-4422-b210-caf112ff4229  dfaabd03-2d40-4422-b210-caf112ff4229   \n",
      "4  dfd0b7ba-c7d3-498e-b455-346301865452  dfd0b7ba-c7d3-498e-b455-346301865452   \n",
      "\n",
      "                       diagnoses           demographic.race demographic.gender  \n",
      "0  [{'age_at_diagnosis': 21369}]                      white             female  \n",
      "1  [{'age_at_diagnosis': 19027}]                      white             female  \n",
      "2  [{'age_at_diagnosis': 10564}]                      white             female  \n",
      "3  [{'age_at_diagnosis': 26535}]  black or african american             female  \n",
      "4  [{'age_at_diagnosis': 22751}]                      white             female  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# API URL\n",
    "url = \"https://api.gdc.cancer.gov/cases\"\n",
    "\n",
    "# API Parameters\n",
    "params = {\n",
    "    \"filters\": json.dumps({\n",
    "        \"op\": \"in\",\n",
    "        \"content\": {\n",
    "            \"field\": \"project.project_id\",\n",
    "            \"value\": [\"TCGA-BRCA\"]\n",
    "        }\n",
    "    }),\n",
    "    \"fields\": \"case_id,demographic.gender,demographic.race,diagnoses.age_at_diagnosis\",\n",
    "    \"size\": \"2000\"\n",
    "}\n",
    "\n",
    "# Fetch data from the API\n",
    "response = requests.get(url, params=params)\n",
    "if response.status_code == 200:\n",
    "    clinical_data = pd.json_normalize(response.json()[\"data\"][\"hits\"])\n",
    "    print(f\"Retrieved {len(clinical_data)} clinical records.\")\n",
    "    print(clinical_data.head())  # Display the first few rows\n",
    "else:\n",
    "    print(f\"Failed to fetch clinical data: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c25e29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age_at_diagnosis\n",
      "0           21369.0\n",
      "1           19027.0\n",
      "2           10564.0\n",
      "3           26535.0\n",
      "4           22751.0\n",
      "Processed features shape: (1082, 3)\n",
      "Generated labels shape: (1082,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract 'age_at_diagnosis' from the nested 'diagnoses' field\n",
    "clinical_data['age_at_diagnosis'] = clinical_data['diagnoses'].apply(\n",
    "    lambda x: x[0]['age_at_diagnosis'] if isinstance(x, list) and len(x) > 0 else None\n",
    ")\n",
    "\n",
    "# Verify the extraction\n",
    "print(clinical_data[['age_at_diagnosis']].head())\n",
    "\n",
    "# Preprocess the clinical data\n",
    "clinical_data['gender'] = LabelEncoder().fit_transform(clinical_data['demographic.gender'])\n",
    "clinical_data['race'] = LabelEncoder().fit_transform(clinical_data['demographic.race'])\n",
    "\n",
    "# Standardize numerical data\n",
    "scaler = StandardScaler()\n",
    "clinical_features = clinical_data[['age_at_diagnosis', 'gender', 'race']].dropna()\n",
    "\n",
    "# Ensure no missing values in features before standardization\n",
    "X_structured = scaler.fit_transform(clinical_features)\n",
    "\n",
    "# Generate simulated labels for demonstration purposes\n",
    "y_structured = np.random.randint(0, 2, len(X_structured))\n",
    "\n",
    "# Output the processed features and labels\n",
    "print(\"Processed features shape:\", X_structured.shape)\n",
    "print(\"Generated labels shape:\", y_structured.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc884d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb7f159316540d38b960b86d27bb7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing DICOM folders:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 923 images.\n",
      "Label distribution: (array([0]), array([923], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "metadata_file_path = r\"D:\\PROJECTS_FINAL\\Cancer Treatment Prediction\\final stuff\\manifest-1732338211342\\metadata.csv\"\n",
    "base_dir = r\"D:\\PROJECTS_FINAL\\Cancer Treatment Prediction\\final stuff\\manifest-1732338211342\"\n",
    "processed_images_dir = os.path.join(base_dir, \"breast_cancer_images_png\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(processed_images_dir, exist_ok=True)\n",
    "\n",
    "# Load metadata\n",
    "metadata = pd.read_csv(metadata_file_path)\n",
    "\n",
    "# Update paths in the metadata to absolute paths\n",
    "metadata['Absolute Path'] = metadata['File Location'].apply(lambda x: os.path.join(base_dir, x.lstrip(\".\\\\\")))\n",
    "\n",
    "IMG_SIZE = (128, 128)\n",
    "images, labels = [], []\n",
    "\n",
    "# Process each folder listed in the metadata\n",
    "for folder_path in tqdm(metadata['Absolute Path'], desc=\"Processing DICOM folders\"):\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if not file_name.endswith(\".dcm\"):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            dicom = dcmread(file_path)\n",
    "            if 'PixelData' not in dicom:\n",
    "                continue\n",
    "\n",
    "            pixel_array = dicom.pixel_array\n",
    "            img = Image.fromarray(pixel_array)\n",
    "            img = img.resize(IMG_SIZE)\n",
    "            img.save(os.path.join(processed_images_dir, f\"{os.path.basename(folder_path)}_{file_name.replace('.dcm', '.png')}\"))\n",
    "\n",
    "            images.append(np.array(img) / 255.0)\n",
    "            labels.append(0)  # Adjust labeling logic as needed\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "X_images = np.array(images).reshape(-1, IMG_SIZE[0], IMG_SIZE[1], 1)\n",
    "y_images = np.array(labels)\n",
    "print(f\"Processed {len(X_images)} images.\")\n",
    "print(f\"Label distribution: {np.unique(y_images, return_counts=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be5a586a-7025-4e7d-92d6-dd3c93aa42b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed images shape: (923, 128, 128, 1)\n",
      "Processed labels shape: (923,)\n"
     ]
    }
   ],
   "source": [
    "X_images = np.array(images).reshape(-1, IMG_SIZE[0], IMG_SIZE[1], 1)\n",
    "y_images = np.array(labels)\n",
    "\n",
    "print(f\"Processed images shape: {X_images.shape}\")\n",
    "print(f\"Processed labels shape: {y_images.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c1dd354-8990-48e8-a1b4-75e2254cfeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (646, 128, 128, 1), (646,)\n",
      "Testing set shape: (277, 128, 128, 1), (277,)\n"
     ]
    }
   ],
   "source": [
    "X_train_img, X_test_img, y_train_img, y_test_img = train_test_split(\n",
    "    X_images, y_images, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train_img.shape}, {y_train_img.shape}\")\n",
    "print(f\"Testing set shape: {X_test_img.shape}, {y_test_img.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c080ae43-bfb5-405b-878e-fdea5c88dfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 267ms/step - accuracy: 0.9440 - loss: 0.1111 - val_accuracy: 1.0000 - val_loss: 5.4883e-11\n",
      "Epoch 2/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 251ms/step - accuracy: 1.0000 - loss: 1.2603e-11 - val_accuracy: 1.0000 - val_loss: 6.1317e-13\n",
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 270ms/step - accuracy: 1.0000 - loss: 1.9849e-15 - val_accuracy: 1.0000 - val_loss: 3.1367e-13\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 264ms/step - accuracy: 1.0000 - loss: 3.4418e-12 - val_accuracy: 1.0000 - val_loss: 2.8711e-13\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 254ms/step - accuracy: 1.0000 - loss: 6.4748e-13 - val_accuracy: 1.0000 - val_loss: 2.8394e-13\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 254ms/step - accuracy: 1.0000 - loss: 7.3937e-14 - val_accuracy: 1.0000 - val_loss: 2.8356e-13\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 279ms/step - accuracy: 1.0000 - loss: 1.6517e-10 - val_accuracy: 1.0000 - val_loss: 2.8344e-13\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 269ms/step - accuracy: 1.0000 - loss: 3.5249e-10 - val_accuracy: 1.0000 - val_loss: 2.8325e-13\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 267ms/step - accuracy: 1.0000 - loss: 6.0682e-13 - val_accuracy: 1.0000 - val_loss: 2.8318e-13\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 275ms/step - accuracy: 1.0000 - loss: 4.4825e-11 - val_accuracy: 1.0000 - val_loss: 2.8315e-13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1db903676e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Add this import\n",
    "\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "datagen.fit(X_train_img)\n",
    "\n",
    "cnn_model.fit(\n",
    "    datagen.flow(X_train_img, y_train_img, batch_size=32),\n",
    "    validation_data=(X_test_img, y_test_img),\n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5afd3cb8-638c-4d1d-9dfa-ec9b783ffa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       277\n",
      "\n",
      "    accuracy                           1.00       277\n",
      "   macro avg       1.00      1.00      1.00       277\n",
      "weighted avg       1.00      1.00      1.00       277\n",
      "\n",
      "Confusion Matrix:\n",
      "[[277]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_pred = (cnn_model.predict(X_test_img) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test_img, y_pred))\n",
    "cm = confusion_matrix(y_test_img, y_pred)\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
